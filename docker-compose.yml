services:
      
  langchain:
    build:
      context: ./langchain  # 현재 디렉토리를 컨텍스트로 사용
      dockerfile: Dockerfile
    container_name: langchain
    # depends_on:
      # - vllm_eeve           # ollama → vllm로 변경
      # - vllm_gemma
    ports:
      - "8001:8001"
    volumes:
      - ./langchain:/app/langchain
    environment:
      - ollama_IP=172.19.0.6   # 필요에 따라 변수명(vllm_IP)으로 변경 가능
      # - CUDA_VISIBLE_DEVICES=0  # GPU 0번만 사용
      # - TF_FORCE_GPU_ALLOW_GROWTH=true  # 메모리 동적 할당
      - OPENAI_API_BASE=http://vllm:8002/v1
      - OPENAI_API_KEY=KIMJINSAN
      - REDIS_HOST=redis  # Redis 호스트명
      - REDIS_PORT=6379
      - MAX_USERS=20      # 최대 사용자 수
      - TTL_SECONDS=3600  # TTL 1시간
    networks:
      - service_network   # 공통 네트워크 사용
    command: uvicorn main:app --host 0.0.0.0 --port 8001 --reload
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
  

  vllm_eeve:
      build:
        context: ./vllm_eeve
        dockerfile: Dockerfile
      container_name: vllm_eeve
      environment:
        # - VLLM_GPU_MEMORY_UTILIZATION=1.0
        - CUDA_VISIBLE_DEVICES=0
        - VLLM_NO_FALLBACK=True
        - NVIDIA_VISIBLE_DEVICES=0
      restart: always
      shm_size: '24gb'
      ports:
        - "8002:8002"
      volumes:
        - ./vllm_eeve:/app/models  # 경로 일치
      networks:
        - service_network

      command: >
        --model /usr/local/bin/models/EEVE-Korean-Instruct-10.8B-v1.0
        --host 0.0.0.0
        --port 8002
        --dtype float16
        --max-model-len 4096
        --gpu-memory-utilization 1.0
        --guided-decoding-backend outlines

      healthcheck:
        test: ["CMD", "curl", "-f", "http://0.0.0.0:8002/v1/models"]
        interval: 30s
        timeout: 5s
        retries: 20
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]


  # vllm_gemma:
  #     build:
  #       context: ./vllm_gemma
  #       dockerfile: Dockerfile
  #     container_name: vllm_gemma
  #     environment:
  #       # - VLLM_GPU_MEMORY_UTILIZATION=1.0      
  #       - CUDA_VISIBLE_DEVICES=0
  #       - VLLM_NO_FALLBACK=True
  #       - NVIDIA_VISIBLE_DEVICES=0
  #     restart: always
  #     # restart: no
  #     shm_size: '24gb'
  #     ports:
  #       - "8022:8022"
  #     volumes:
  #       - ./vllm_gemma:/app/models  # 경로 일치
  #     networks:
  #       - service_network

  #     command: >
  #       --model /usr/local/bin/models/gemma-3-4b-it
  #       --host 0.0.0.0
  #       --port 8022
  #       --dtype float16
  #       --max-model-len 8192
  #       --gpu-memory-utilization 1.0
  #       --guided-decoding-backend outlines
        
  #     healthcheck:
  #       test: ["CMD", "curl", "-f", "http://0.0.0.0:8022/v1/models"]
  #       interval: 30s
  #       timeout: 5s
  #       retries: 20
  #     deploy:
  #       resources:
  #         reservations:
  #           devices:
  #             - driver: nvidia
  #               count: 1
  #               capabilities: [gpu]

  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - service_network
    
networks:
  service_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.19.0.0/16




# volumes:
#   etcd-data:
#   minio-data:

  # locust-master:
  #     image: locustio/locust
  #     volumes:
  #       - ./locust/locustfile.py:/locustfile.py
  #     ports:
  #       - "8089:8089"  # 웹 UI 포트
  #       - "5557:5557"  # 마스터-워커 통신 포트
  #     command: -f /locustfile.py --master --host=http://langchain:8001 --loglevel DEBUG
  #     # healthcheck:
  #     #   test: ["CMD", "nc", "-z", "localhost", "5557"]  # 5557 포트가 열렸는지 확인
  #     #   interval: 5s
  #     #   timeout: 100s
  #     #   retries: 10
  #     networks:
  #       - service_network

  # locust-worker:
  #   image: locustio/locust
  #   volumes:
  #     - ./locust/locustfile.py:/locustfile.py
  #   command: -f /locustfile.py --worker --master-host=locust-master --loglevel DEBUG
  #   # depends_on:
  #   #   locust-master:
  #   #     condition: service_healthy  # locust-master가 healthy 상태일 때 시작
  #   networks:
  #     - service_network

  # etcd:
  #   container_name: milvus-etcd
  #   image: quay.io/coreos/etcd:v3.5.14
  #   environment:
  #     - ETCD_AUTO_COMPACTION_MODE=revision
  #     - ETCD_AUTO_COMPACTION_RETENTION=1000
  #     - ETCD_QUOTA_BACKEND_BYTES=4294967296
  #     - ETCD_SNAPSHOT_COUNT=50000
  #   volumes:
  #     - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
  #   command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
  #   healthcheck:
  #     test: ["CMD", "etcdctl", "endpoint", "health"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     - service_network

  # minio:
  #   container_name: milvus-minio
  #   image: minio/minio:RELEASE.2023-03-20T20-16-18Z
  #   environment:
  #     MINIO_ACCESS_KEY: minioadmin
  #     MINIO_SECRET_KEY: minioadmin
  #   ports:
  #     - "9001:9001"
  #     - "9000:9000"
  #   volumes:
  #     - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
  #   command: minio server /minio_data --console-address ":9001"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     - service_network

  # standalone:
  #   container_name: milvus-standalone
  #   image: milvusdb/milvus:v2.3.0
  #   command: ["milvus", "run", "standalone"]
  #   security_opt:
  #   - seccomp:unconfined
  #   environment:
  #     MINIO_REGION: us-east-1
  #     ETCD_ENDPOINTS: etcd:2379
  #     MINIO_ADDRESS: minio:9000
  #     LOG_LEVEL: warn
  #   volumes:
  #     - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
  #     interval: 30s
  #     start_period: 90s
  #     timeout: 20s
  #     retries: 3
  #   ports:
  #     - "19530:19530"
  #     - "9091:9091"
  #   depends_on:
  #     - "etcd"
  #     - "minio"
  #   networks:
  #     service_network:
  #       ipv4_address: 172.19.0.6

  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   build:
  #     context: ./ollama
  #     dockerfile: Dockerfile
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ./llama_models:/models
  #   environment:
  #     - OLLAMA_API_PORT=11434
  #     - OLLAMA_HOST=0.0.0.0:11434
  #     - CUDA_VISIBLE_DEVICES=0  # GPU 0번만 사용
  #     - TF_FORCE_GPU_ALLOW_GROWTH=true  # 메모리 동적 할당
  #   networks:
  #     - service_network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]


  # vllm:
  #     build:
  #       context: ./vllm
  #       dockerfile: Dockerfile
  #     environment:
  #       - VLLM_GPU_MEMORY_UTILIZATION=1.0
  #       - CUDA_VISIBLE_DEVICES=0
  #       - VLLM_NO_FALLBACK=True
  #       - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
  #     restart: always
  #     shm_size: '50gb'
  #     ports:
  #       - "8002:8002"
  #     volumes:
  #       - ./vllm:/app/models  # 경로 일치
  #     networks:
  #       - service_network
  #     # NOTE_250311 : run_server.py 사용을 위한 테스트
  #     #               이렇게 자세하게 적어서 api_server를 열어놓기.
  #     #               여러개의 모델을 사용하려면 port를 나눠서 열어야 함.
  #     # command: python3 -m vllm.entrypoints.openai.api_server --port 8002 --host 0.0.0.0 --model "/usr/local/bin/models/EEVE-Korean-Instruct-10.8B-v1.0" --dtype float16 --gpu-memory-utilization 1.0 --max-model-len 4096 --guided-decoding-backend outlines
  #     command: python3 -m vllm.entrypoints.openai.api_server --port 8002 --host 0.0.0.0 --model "/usr/local/bin/models/EEVE-Korean-Instruct-10.8B-v1.0" --dtype float16 --gpu-memory-utilization 1.0 --max-model-len 4096 --max-num-batched-tokens 8192 --max-num-seqs 50 --tensor-parallel-size 1 --guided-decoding-backend outlines
  #     # command: python3 -m vllm.entrypoints.openai.api_server --port 8002 --host 0.0.0.0 --model "/usr/local/bin/models/EEVE-Korean-Instruct-10.8B-v1.0" --dtype float16 --gpu-memory-utilization 1.0 --max-model-len 4096
  #     # command: python3 /app/models/src/run_server.py
  #     healthcheck:
  #       test: ["CMD", "curl", "-f", "http://0.0.0.0:8002/v1/models"]
  #       interval: 30s
  #       timeout: 5s
  #       retries: 20
  #     deploy:
  #       resources:
  #         reservations:
  #           devices:
  #             - driver: nvidia
  #               count: 1
  #               capabilities: [gpu]