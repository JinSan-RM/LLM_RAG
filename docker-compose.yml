services:
  # etcd:
  #   container_name: milvus-etcd
  #   image: quay.io/coreos/etcd:v3.5.14
  #   environment:
  #     - ETCD_AUTO_COMPACTION_MODE=revision
  #     - ETCD_AUTO_COMPACTION_RETENTION=1000
  #     - ETCD_QUOTA_BACKEND_BYTES=4294967296
  #     - ETCD_SNAPSHOT_COUNT=50000
  #   volumes:
  #     - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
  #   command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
  #   healthcheck:
  #     test: ["CMD", "etcdctl", "endpoint", "health"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     - service_network

  # minio:
  #   container_name: milvus-minio
  #   image: minio/minio:RELEASE.2023-03-20T20-16-18Z
  #   environment:
  #     MINIO_ACCESS_KEY: minioadmin
  #     MINIO_SECRET_KEY: minioadmin
  #   ports:
  #     - "9001:9001"
  #     - "9000:9000"
  #   volumes:
  #     - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
  #   command: minio server /minio_data --console-address ":9001"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3
  #   networks:
  #     - service_network

  # standalone:
  #   container_name: milvus-standalone
  #   image: milvusdb/milvus:v2.3.0
  #   command: ["milvus", "run", "standalone"]
  #   security_opt:
  #   - seccomp:unconfined
  #   environment:
  #     MINIO_REGION: us-east-1
  #     ETCD_ENDPOINTS: etcd:2379
  #     MINIO_ADDRESS: minio:9000
  #     LOG_LEVEL: warn
  #   volumes:
  #     - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
  #     interval: 30s
  #     start_period: 90s
  #     timeout: 20s
  #     retries: 3
  #   ports:
  #     - "19530:19530"
  #     - "9091:9091"
  #   depends_on:
  #     - "etcd"
  #     - "minio"
  #   networks:
  #     service_network:
  #       ipv4_address: 172.19.0.6

  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   build:
  #     context: ./ollama
  #     dockerfile: Dockerfile
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ./llama_models:/models
  #   environment:
  #     - OLLAMA_API_PORT=11434
  #     - OLLAMA_HOST=0.0.0.0:11434
  #     - CUDA_VISIBLE_DEVICES=0  # GPU 0번만 사용
  #     - TF_FORCE_GPU_ALLOW_GROWTH=true  # 메모리 동적 할당
  #   networks:
  #     - service_network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]
      
  langchain:
    build:
      context: ./langchain  # 현재 디렉토리를 컨텍스트로 사용
      dockerfile: Dockerfile
    container_name: langchain
    depends_on:
      - vllm_eeve           # vllm → vllm_eeve로 변경
      # - vllm_gemma
    ports:
      - "8001:8001"
    volumes:
      - ./langchain:/app/langchain
    environment:
      - ollama_IP=172.19.0.6   # 필요에 따라 변수명(vllm_IP)으로 변경 가능
      - CUDA_VISIBLE_DEVICES=0  # GPU 0번만 사용
      - TF_FORCE_GPU_ALLOW_GROWTH=true  # 메모리 동적 할당
      - OPENAI_API_BASE=http://vllm_eeve:8002/v1
      - OPENAI_API_KEY=KIMJINSAN
      # gemma용은 나중에 추가가
    networks:
      - service_network   # 공통 네트워크 사용
    command: uvicorn main:app --host 0.0.0.0 --port 8001 --reload
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

#   vllm:
#     image: vllm/vllm-openai:latest
#     environment:
#       - VLLM_GPU_MEMORY_UTILIZATION=0.8
#       - VLLM_MAX_SEQ_LEN=2048
#       - CUDA_VISIBLE_DEVICES=0
#       - VLLM_NO_FALLBACK=True    
#     build:
#       context: ./vllm
#       dockerfile: Dockerfile
#     restart: always
#     shm_size: '72gb'
#     ports:
#       - "8002:8002"
#     volumes:
#       - ./vllm:/app/models
#     networks:
#       - service_network   # h2ogpt → service_network로 변경
#     entrypoint: python3
#     tty: true
#     stdin_open: true
#     command: -m vllm.entrypoints.openai.api_server --port=8002 --host=0.0.0.0 ${LANGCHAIN_VLLM_ARGS} --model "/usr/local/bin/models/EEVE-Korean-Instruct-10.8B-v1.0" --dtype float16
#     healthcheck:
#       test: [ "CMD", "curl", "-f", "http://0.0.0.0:8002/v1/models" ]
#       interval: 30s
#       timeout: 5s
#       retries: 20
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: 1
#               capabilities: [gpu]

  vllm_eeve:
      build:
        context: ./vllm_eeve
        dockerfile: Dockerfile
      container_name: vllm_eeve
      environment:
        - VLLM_GPU_MEMORY_UTILIZATION=1.0
        - CUDA_VISIBLE_DEVICES=0
        - VLLM_NO_FALLBACK=True
      restart: always
      shm_size: '72gb'
      ports:
        - "8002:8002"
      volumes:
        - ./vllm_eeve:/app/models  # 경로 일치
      networks:
        - service_network
      # NOTE_250311 : run_server.py 사용을 위한 테스트
      #               이렇게 자세하게 적어서 api_server를 열어놓기.
      #               여러개의 모델을 사용하려면 port를 나눠서 열어야 함.
      # NOTE 250319 gemma-2-2b-it 테스트 및 복수 모델 테스트트
      command: python3 -m vllm.entrypoints.openai.api_server --port 8002 --host 0.0.0.0 --model "/usr/local/bin/models/EEVE-Korean-Instruct-10.8B-v1.0" --dtype float16 --gpu-memory-utilization 1.0 --max-model-len 4096 --guided-decoding-backend outlines

      healthcheck:
        test: ["CMD", "curl", "-f", "http://0.0.0.0:8002/v1/models"]
        interval: 30s
        timeout: 5s
        retries: 20
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]

  # vllm_gemma:
  #     build:
  #       context: ./vllm_gemma
  #       dockerfile: Dockerfile
  #     container_name: vllm_gemma
  #     environment:
  #       - VLLM_GPU_MEMORY_UTILIZATION=0.3
  #       - CUDA_VISIBLE_DEVICES=0
  #       - VLLM_NO_FALLBACK=True
  #     # restart: always
  #     restart: no
  #     shm_size: '72gb'
  #     ports:
  #       - "8022:8022"
  #     volumes:
  #       - ./vllm_gemma:/app/models  # 경로 일치
  #     networks:
  #       - service_network
  #     # NOTE_250311 : run_server.py 사용을 위한 테스트
  #     #               이렇게 자세하게 적어서 api_server를 열어놓기.
  #     #               여러개의 모델을 사용하려면 port를 나눠서 열어야 함.
  #     # NOTE 250319 gemma-2-2b-it 테스트 및 복수 모델 테스트
  #     # command: python3 -m vllm.entrypoints.openai.api_server --port 8022 --host 0.0.0.0 --model '/usr/local/bin/models/gemma-2-2b-it' --dtype float16 --gpu-memory-utilization 0.4 --max-model-len 8192 --guided-decoding-backend outlines
  #     command: python3 -m vllm.entrypoints.openai.api_server --port 8022 --host 0.0.0.0 --model '/usr/local/bin/models/gemma-2-9b-it' --dtype float16 --gpu-memory-utilization 1.0 --max-model-len 8192 --guided-decoding-backend outlines
  #     # NOTE 250320 gemma-2-9b-it 테스트 및 복수 모델 테스트
  #     # command: python3 -m vllm.entrypoints.openai.api_server --port 8022 --host 0.0.0.0 --model '/usr/local/bin/models/EXAONE-Deep-2.4B' --dtype float16 --gpu-memory-utilization 1.0 --max-model-len 8192 --guided-decoding-backend outlines --trust-remote-code
  #     # command: bash -c "python3 -m vllm.entrypoints.openai.api_server --port 8022 --host 0.0.0.0 --model '/usr/local/bin/models/gemma-3-4b-it' --dtype float16 --gpu-memory-utilization 1.0 --max-model-len 8192 --guided-decoding-backend outlines || sleep infinity"

  #     healthcheck:
  #       test: ["CMD", "curl", "-f", "http://0.0.0.0:8022/v1/models"]
  #       interval: 30s
  #       timeout: 5s
  #       retries: 20
  #     deploy:
  #       resources:
  #         reservations:
  #           devices:
  #             - driver: nvidia
  #               count: 1
  #               capabilities: [gpu]  


    # 여러개의 모델을 사용하려면 port를 나눠서 열어야 함.
    # NOTE 250319 gemma-2-2b-it 테스트 및 복수 모델 테스트트

networks:
  service_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.19.0.0/16

# volumes:
#   etcd-data:
#   minio-data:
